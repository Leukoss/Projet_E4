{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc24c927b60b438",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3a249c70ed29e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:08:52.177031800Z",
     "start_time": "2024-04-22T20:08:27.196354800Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2                                          # state of the art computer vision algorithms library\n",
    "import numpy as np                                  # fundamental package for scientific computing\n",
    "import mediapipe as mp                              # pose estimation through mp\n",
    "import pyrealsense2 as rs                           # Intel RealSense cross-platform open-source API\n",
    "import socket                                       # Local connection to Unity\n",
    "from tensorflow.keras.models import load_model      # Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058f3db92b8463",
   "metadata": {},
   "source": [
    "# Setup Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cf21c2450db88d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:09:37.569517300Z",
     "start_time": "2024-04-22T20:09:37.409897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize to retrieve the camera flow\n",
    "pipe = rs.pipeline()\n",
    "cfg = rs.config()\n",
    "\n",
    "# Define the format of both stream\n",
    "cfg.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "cfg.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "# Align both cameras\n",
    "align = rs.align(rs.stream.color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef65bbeb1bdae9",
   "metadata": {},
   "source": [
    "# Setup Calibration Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b5ca6c397a5a015",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:28:04.196065300Z",
     "start_time": "2024-04-22T20:28:04.154147800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the parameters for cornerSubPix\n",
    "winSize = (5, 5)  # Size of the window for searching sub-pixel corner\n",
    "zeroZone = (-1, -1)  # No search zone restriction around the corner\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)  # Termination criteria\n",
    "\n",
    "# Define the size of our calibration target\n",
    "pattern_size = (7, 7)\n",
    "\n",
    "objp = np.zeros((1, pattern_size[0] * pattern_size[1], 3), np.float32)\n",
    "objp[0,:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)\n",
    "\n",
    "images = []\n",
    "images_corners = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6aad4",
   "metadata": {},
   "source": [
    "# Setup TCP Connection to Unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e157607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TCP/IP socket\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind(('localhost', 9999))  # Bind to localhost on port 9999\n",
    "server_socket.listen(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8b7ed",
   "metadata": {},
   "source": [
    "# Setup Hand Tracking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b8f0e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['okay', 'peace', 'thumbs up', 'thumbs down', 'call me', 'stop', 'rock', 'live long', 'fist', 'smile']\n"
     ]
    }
   ],
   "source": [
    "# initialize mediapipe\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the gesture recognizer model\n",
    "model = load_model('mp_hand_gesture')\n",
    "\n",
    "# Load class names\n",
    "f = open('gesture.names', 'r')\n",
    "classNames = f.read().split('\\n')\n",
    "f.close()\n",
    "print(classNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c7af8e0a71ac8",
   "metadata": {},
   "source": [
    "# Calibrated Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d979b831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Unity connection...\n",
      "Connected to Unity: ('127.0.0.1', 52257)\n",
      "Target found\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Connection to Unity ended.\n"
     ]
    }
   ],
   "source": [
    "old_z = 0 # Save last z coordinate, will be used if none is determined (default value = 0)\n",
    "\n",
    "# Connection to Unity\n",
    "print(\"Waiting for Unity connection...\")\n",
    "connection, address = server_socket.accept()\n",
    "print(\"Connected to Unity:\", address)\n",
    "\n",
    "\n",
    "# Start the capture\n",
    "pipe.start(cfg)\n",
    "\n",
    "while True:\n",
    "    # Wait for a coherent pair of frames \"depth and color\" and align \n",
    "    # them\n",
    "    frame = pipe.wait_for_frames()\n",
    "    aligned_frame = align.process(frame)\n",
    "\n",
    "    # Retrieve the color flow\n",
    "    color_frame = frame.get_color_frame()\n",
    "    \n",
    "    if not color_frame:\n",
    "        continue\n",
    "\n",
    "    # Show the RGB frame\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    cv2.imshow('Color Image', color_image)\n",
    "\n",
    "    # Find the target corner in the frame\n",
    "    found, corners = cv2.findChessboardCorners(color_image, \n",
    "        pattern_size, flags=cv2.CALIB_CB_ADAPTIVE_THRESH + \n",
    "        cv2.CALIB_CB_FAST_CHECK +\n",
    "        cv2.CALIB_CB_NORMALIZE_IMAGE)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'): break\n",
    "\n",
    "    if found:\n",
    "        print(\"Target found\")\n",
    "        # Draw the shape of the target\n",
    "        images.append(objp)\n",
    "        cv2.drawChessboardCorners(color_image, pattern_size, \n",
    "            corners, found)\n",
    "        \n",
    "        # Change RBG for Grayscale\n",
    "        gray_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply cornerSubPix on the gray picture\n",
    "        corners_refined = cv2.cornerSubPix(gray_image, corners, \n",
    "            winSize, zeroZone, criteria)\n",
    "\n",
    "        # Add the corner in the list\n",
    "        images_corners.append(corners_refined)\n",
    "\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "retval, cameraMatrix, distCoeffs, rvecs, tvecs = cv2.calibrateCamera(images, images_corners, gray_image.shape[::-1],None,None)\n",
    "\n",
    "# Real size of the target (cm)\n",
    "mire_size_cm = 23.3\n",
    "\n",
    "# Focal in pixels accros the x-axis\n",
    "fx = cameraMatrix[0, 0]  \n",
    "taille_pixel_x = 1 / fx\n",
    "# Focal in pixels accros the y-axis\n",
    "fy = cameraMatrix[1, 1]  \n",
    "taille_pixel_y = 1 / fy\n",
    "\n",
    "decimation = rs.decimation_filter()\n",
    "decimation.set_option(rs.option.filter_magnitude, 1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "  # Start the capture\n",
    "  #pipe.start(cfg)\n",
    "\n",
    "  while True:\n",
    "    # Wait for a coherent pair of frames \"depth and color\" and align them\n",
    "    frame = pipe.wait_for_frames()\n",
    "    aligned_frame = align.process(frame)\n",
    "\n",
    "    # Retrieve the depth and color flow\n",
    "    depth_frame = frame.get_depth_frame()\n",
    "    color_frame = frame.get_color_frame()\n",
    "\n",
    "    # Apply smothering filter\n",
    "    spatial = rs.spatial_filter()\n",
    "    spatial.set_option(rs.option.filter_magnitude, 5)\n",
    "    spatial.set_option(rs.option.filter_smooth_alpha, 1)\n",
    "    spatial.set_option(rs.option.filter_smooth_delta, 50)\n",
    "\n",
    "    # Apply filters to fill the holes\n",
    "    hole_filling = rs.hole_filling_filter()\n",
    "\n",
    "    # Retrieve the images from both flow (colorized depth)\n",
    "    colorizer = rs.colorizer()\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    decimated_depth = decimation.process(depth_frame)\n",
    "    smoothed_depth = spatial.process(decimated_depth)\n",
    "    filled_depth = hole_filling.process(smoothed_depth)\n",
    "    depth_image = np.asanyarray(filled_depth.get_data())\n",
    "    colorized_depth_frame = np.asanyarray(colorizer.colorize(filled_depth).get_data())\n",
    "\n",
    "    # Add MP on the colored image\n",
    "    color_image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "    color_image_rgb.flags.writeable = False\n",
    "\n",
    "    results = pose.process(color_image_rgb)\n",
    "\n",
    "    color_image_rgb.flags.writeable = True\n",
    "    color_image_rgb = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Render detections\n",
    "    mp_drawing.draw_landmarks(color_image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                               )\n",
    "    \n",
    "    landmarks_to_send = '' # String containing the x,y,z coordinates of all the landmarks to send per frame\n",
    "    # Retrieve the coordinates for each landmark\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            x = int(landmark.x * color_image.shape[1])\n",
    "            y = int(landmark.y * color_image.shape[0]) \n",
    "            \n",
    "            # Ensure that the calculated pixel coordinates are within the bounds of the depth frame\n",
    "            if 0 <= y < color_image.shape[0] and 0 <= x < color_image.shape[1]:\n",
    "                depth_value = depth_frame.get_distance(x,y)\n",
    "                old_z = depth_value\n",
    "            else :\n",
    "                depth_value = 0\n",
    "                \n",
    "            text = f\"X={x*taille_pixel_x:.2f}, Y={y*taille_pixel_y:.2f}, Z={depth_value:.2f}\"\n",
    "            cv2.putText(color_image_rgb, text, (x + 10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Add the x,y,z coordinates of the landmark to the string of landmarks to send\n",
    "            landmarks_to_send += f\"{int(x*taille_pixel_x*100)},{int(y*taille_pixel_y*100)},{int(depth_value*100)},\" \n",
    "        \n",
    "        # Send pose data to Unity\n",
    "        connection.sendall(landmarks_to_send[:-1].encode())  # Send pose data to Unity\n",
    "    \n",
    "    \n",
    "    ################################ hand gesture #####################\n",
    "    framergb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = hands.process(framergb)  \n",
    "    className = ''\n",
    "\n",
    "    # post process the result\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            for lm in handslms.landmark:\n",
    "                # print(id, lm)\n",
    "                lmx = int(lm.x * x)\n",
    "                lmy = int(lm.y * y)\n",
    "                landmarks.append([lmx, lmy])\n",
    "            # Drawing landmarks on frames\n",
    "            mpDraw.draw_landmarks(color_image, handslms, mpHands.HAND_CONNECTIONS)\n",
    "            # Predict gesture\n",
    "            prediction = model.predict([landmarks])\n",
    "            # print(prediction)\n",
    "            classID = np.argmax(prediction)\n",
    "            className = classNames[classID]\n",
    "    # show the prediction on the frame\n",
    "    cv2.putText(color_image, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   1, (0,0,255), 2, cv2.LINE_AA)\n",
    "    #######################################################################\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Depth\", colorized_depth_frame)\n",
    "    cv2.imshow(\"Color\", color_image)\n",
    "    cv2.imshow(\"MediaPipe\", color_image_rgb)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "      break\n",
    "\n",
    "  pipe.stop()\n",
    "  cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "connection.close()\n",
    "server_socket.close()\n",
    "print('Connection to Unity ended.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06317681",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stop() cannot be called before start()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stop() cannot be called before start()"
     ]
    }
   ],
   "source": [
    "pipe.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
